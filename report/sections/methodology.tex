The experimental pipeline was divided into four primary stages: indexing,
feature extraction, prototype initialisation, and iterative MapReduce execution.

\begin{enumerate}
    \item \textbf{Indexing and Pre-processing:} Documents were indexed in Elasticsearch using a \texttt{letter} tokeniser and a Snowball stemmer to reduce words to their linguistic roots. We applied a length filter to discard tokens with fewer than 2 or more than 10 characters to eliminate noise.
    \item \textbf{Feature Extraction:} Binary term vectors were generated by selecting a vocabulary of 250 words. To ensure cluster discriminability, we applied frequency filters: words appearing in more than 20\% of documents (potential stop words) or fewer than 5\% (outliers) were excluded.
    \item \textbf{Similarity Metric:} We implemented the Generalised Jaccard similarity to compare binary document vectors against weighted prototype vectors. This was calculated as:
    \[ J(doc, prot) = \frac{doc \cdot prot}{||doc||_2^2 + ||prot||_2^2 - doc \cdot prot} \]
    Our implementation used an efficient two-pointer approach, thereby using the alphabetical ordering of the vocabulary to reduce computational cost.
    \item \textbf{MapReduce Execution:} The \texttt{MRKmeansStep} class defined the Mapper for document assignment and the Reducer for prototype re-computation. The control script, \texttt{MRKmeans.py}, performed 20 iterations, passing the output of each step as the input for the next.
\end{enumerate}